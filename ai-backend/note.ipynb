{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 0. INSTALLS (Colab only - comment out for local use)\n",
    "# ==============================================================================\n",
    "# Uncomment the following lines if running in Google Colab or hosted environment:\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh\n",
    "# !pip install fastapi uvicorn pyngrok requests boto3 python-multipart aiofiles langchain langchain-community chromadb sentence-transformers PyMuPDF langchain-huggingface langchain-chroma langchain-ollama langchain-experimental flashrank pydantic python-dotenv\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. IMPORTS\n",
    "# ==============================================================================\n",
    "import os, signal, psutil, gc, time, sys, subprocess, threading, requests, tempfile, asyncio, json, base64, random\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- FastAPI & Server ---\n",
    "from fastapi import FastAPI, UploadFile, Form, Request, HTTPException\n",
    "from fastapi.responses import JSONResponse, StreamingResponse, FileResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# --- LangChain Core ---\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.globals import set_verbose\n",
    "from pydantic import BaseModel, Field\n",
    "import fitz\n",
    "\n",
    "# --- Colab Support ---\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURATION\n",
    "# ==============================================================================\n",
    "print(\"Loading configuration...\")\n",
    "\n",
    "# --- API Keys & Models ---\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        NGROK_AUTHTOKEN = \"32eB7tLSQoICKJD4JSQuJ9lWea6_7U5ndjtQCVaWnPLEc4Mws\"\n",
    "        PROGRESS_SERVICE_URL = os.environ.get(\"PROGRESS_SERVICE_URL\", \"https://localdocu-progress.vercel.app\")\n",
    "    except Exception:\n",
    "        print(\"WARNING: Could not load from Colab secrets, falling back to environment variables.\")\n",
    "        NGROK_AUTHTOKEN = \"32eB7tLSQoICKJD4JSQuJ9lWea6_7U5ndjtQCVaWnPLEc4Mws\"\n",
    "        PROGRESS_SERVICE_URL = os.environ.get(\"PROGRESS_SERVICE_URL\", \"https://localdocu-progress.vercel.app\")\n",
    "else:\n",
    "    NGROK_AUTHTOKEN = \"32eB7tLSQoICKJD4JSQuJ9lWea6_7U5ndjtQCVaWnPLEc4Mws\"\n",
    "    PROGRESS_SERVICE_URL = os.environ.get(\"PROGRESS_SERVICE_URL\", \"https://localdocu-progress.vercel.app\")\n",
    "\n",
    "if not NGROK_AUTHTOKEN or NGROK_AUTHTOKEN == \"YOUR_NGROK_AUTHTOKEN\":\n",
    "    print(\"WARNING: NGROK_AUTHTOKEN not configured properly. Set it in .env file.\")\n",
    "\n",
    "OLLAMA_MODEL = os.environ.get(\"OLLAMA_MODEL\", \"gemma3:1b\")\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "# --- Persistent Storage Paths (Hierarchical) ---\n",
    "PERSIST_BASE = os.path.abspath(\"./chroma_store\")\n",
    "SUMMARY_STORE_PATH = os.path.join(PERSIST_BASE, \"summary_store\")\n",
    "DETAILED_STORE_PATH = os.path.join(PERSIST_BASE, \"detailed_store\")\n",
    "IMAGE_STORE = os.path.abspath(\"./image_store\")\n",
    "\n",
    "os.makedirs(SUMMARY_STORE_PATH, exist_ok=True)\n",
    "os.makedirs(DETAILED_STORE_PATH, exist_ok=True)\n",
    "os.makedirs(IMAGE_STORE, exist_ok=True)\n",
    "\n",
    "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\"}\n",
    "\n",
    "def post_progress(document_id: str, status: str, progress: int = 0, **kwargs):\n",
    "    \"\"\"Post progress update to progress tracking service.\"\"\"\n",
    "    try:\n",
    "        payload = {\"documentId\": document_id, \"status\": status, \"progress\": progress, **kwargs}\n",
    "        threading.Thread(target=lambda: requests.post(f\"{PROGRESS_SERVICE_URL}/progress\", json=payload, timeout=10), daemon=True).start()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# --- Global Reusable Components ---\n",
    "EMBEDDINGS_MODEL = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "GLOBAL_RERANKER = FlashrankRerank(top_n=5) # Default re-ranker\n",
    "\n",
    "def safe_metadata_value(value):\n",
    "    \"\"\"Convert unsupported metadata types (lists, dicts, objects) to JSON strings or plain strings.\n",
    "\n",
    "    Chroma/Chromadb requires metadata values to be primitive types (str, int, float, bool, None) or SparseVector.\n",
    "    We ensure we never store lists/dicts directly by serializing them.\n",
    "    \"\"\"\n",
    "    # Primitive safe types\n",
    "    if value is None or isinstance(value, (str, int, float, bool)):\n",
    "        return value\n",
    "    # For lists and dicts, try JSON serialization\n",
    "    try:\n",
    "        if isinstance(value, (list, dict)):\n",
    "            return json.dumps(value)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: convert to string\n",
    "    try:\n",
    "        return str(value)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def sanitize_metadata(metadata: dict) -> dict:\n",
    "    \"\"\"Return a sanitized metadata dict where every value is a primitive or None.\n",
    "\n",
    "    - If metadata is not a dict, returns empty dict.\n",
    "    - For nested dict/list, serializes to JSON string.\n",
    "    \"\"\"\n",
    "    if not isinstance(metadata, dict):\n",
    "        return {}\n",
    "    sanitized = {}\n",
    "    for k, v in metadata.items():\n",
    "        try:\n",
    "            sanitized[k] = safe_metadata_value(v)\n",
    "        except Exception:\n",
    "            sanitized[k] = None\n",
    "    return sanitized\n",
    "\n",
    "def is_image_file(filename: str) -> bool:\n",
    "    try:\n",
    "        return Path(filename).suffix.lower() in IMAGE_EXTENSIONS\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_public_url() -> str:\n",
    "    if os.environ.get(\"PUBLIC_URL\"):\n",
    "        return os.environ.get(\"PUBLIC_URL\")\n",
    "    return globals().get(\"public_url\", \"http://localhost:8000\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. NEW: Pydantic Models for Structured Output\n",
    "# ==============================================================================\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    \"\"\"Pydantic model for a single citation reference.\"\"\"\n",
    "    id: str = Field(..., description=\"The citation ID, e.g., '1', '2'.\")\n",
    "    title: str = Field(..., description=\"The title of the source document.\")\n",
    "    source: str = Field(..., description=\"The source URL or filename.\")\n",
    "    page: int = Field(default=0, description=\"The page number in the document.\")\n",
    "    snippet: str = Field(default=\"\", description=\"A short text snippet from the source.\")\n",
    "\n",
    "class AIAnswer(BaseModel):\n",
    "    \"\"\"Pydantic model for the LLM's structured answer.\"\"\"\n",
    "    answer: str = Field(..., description=\"The detailed answer to the user's query, with IEEE-style citations like [1], [2].\")\n",
    "    references: List[Reference] = Field(..., description=\"A list of Reference objects used in the answer.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. NEW: Citation Deduplication Utility\n",
    "# ==============================================================================\n",
    "\n",
    "def deduplicate_references_and_update_answer(answer: str, references: List[Reference]) -> tuple[str, List[Reference]]:\n",
    "    \"\"\"\n",
    "    Deduplicates references and updates the answer text to maintain correct reference numbering.\n",
    "    \"\"\"\n",
    "    if not references:\n",
    "        return answer, []\n",
    "\n",
    "    unique_refs = {}\n",
    "    id_mapping = {}\n",
    "\n",
    "    # Create unique references and map old IDs to new IDs\n",
    "    new_id_counter = 1\n",
    "    for ref in references:\n",
    "        if ref.source not in unique_refs:\n",
    "            new_id = str(new_id_counter)\n",
    "            unique_refs[ref.source] = Reference(id=new_id, title=ref.title, source=ref.source, page=ref.page, snippet=ref.snippet)\n",
    "            new_id_counter += 1\n",
    "\n",
    "        id_mapping[ref.id] = unique_refs[ref.source].id\n",
    "\n",
    "    updated_answer = answer\n",
    "\n",
    "    # Sort keys by length (desc) to replace \"[10]\" before \"[1]\"\n",
    "    sorted_old_ids = sorted(id_mapping.keys(), key=len, reverse=True)\n",
    "\n",
    "    for old_id in sorted_old_ids:\n",
    "        new_id = id_mapping[old_id]\n",
    "        # Replace citations (e.g., [1], [2], etc.)\n",
    "        updated_answer = updated_answer.replace(f'[{old_id}]', f'[{new_id}]')\n",
    "\n",
    "    return updated_answer, list(unique_refs.values())\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. SYSTEM & OLLAMA UTILS\n",
    "# ==============================================================================\n",
    "\n",
    "def stream_logs(proc, name):\n",
    "    for line in iter(proc.stdout.readline, b''):\n",
    "        sys.stdout.write(f\"[{name}] {line.decode()}\")\n",
    "        sys.stdout.flush()\n",
    "    for line in iter(proc.stderr.readline, b''):\n",
    "        sys.stdout.write(f\"[{name}-ERR] {line.decode()}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def start_ollama_service():\n",
    "    ollama_proc = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    threading.Thread(target=stream_logs, args=(ollama_proc, \"Ollama\"), daemon=True).start()\n",
    "    print(\"Starting Ollama service...\")\n",
    "    for _ in range(40):\n",
    "        try:\n",
    "            if requests.get(OLLAMA_URL).status_code == 200:\n",
    "                print(\"Ollama is running locally!\\n\")\n",
    "                return True\n",
    "        except:\n",
    "            time.sleep(2)\n",
    "    raise RuntimeError(\"Ollama failed to start in time.\")\n",
    "\n",
    "def generate_image_summary(image_path: str, model: str = \"llava\") -> str:\n",
    "    \"\"\"Generate a detailed description of an image using a vision model.\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read()).decode()\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/api/generate\",\n",
    "            json={\"model\": model, \"prompt\": \"Describe this image in detail, including any text, objects, colors, and context.\", \"images\": [image_data], \"stream\": False},\n",
    "            timeout=120\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"No description available\")\n",
    "        else:\n",
    "            return f\"Error generating summary: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. SHARED LLM & RAG PROMPT LOGIC (MODIFIED)\n",
    "# ==============================================================================\n",
    "\n",
    "def get_llm(model_name: str):\n",
    "    \"\"\"Unified function to get an LLM instance.\"\"\"\n",
    "    # Using a model known to be good at JSON mode\n",
    "    return OllamaLLM(model=model_name, format=\"json\", temperature=0)\n",
    "\n",
    "def generate_with_llm(prompt: str, model_name: str):\n",
    "    \"\"\"Unified function to invoke an LLM for *non-structured* text.\"\"\"\n",
    "    # Use a basic model for simple generation\n",
    "    llm = OllamaLLM(model=model_name, temperature=0.1)\n",
    "\n",
    "    resp = llm.invoke(prompt)\n",
    "    return getattr(resp, \"content\", str(resp))\n",
    "\n",
    "def _format_chunks_for_prompt(chunks: List[Document]) -> str:\n",
    "    \"\"\"Formats retrieved chunks into the string format\"\"\"\n",
    "    context_strings = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if not hasattr(chunk, 'metadata') or not isinstance(chunk.metadata, dict):\n",
    "            print(f\"Warning: chunk {i} has invalid metadata, skipping\")\n",
    "            continue\n",
    "        source = chunk.metadata.get(\"source\", \"N/A\")\n",
    "        title = chunk.metadata.get(\"title\", chunk.metadata.get(\"filename\", source))\n",
    "        page = chunk.metadata.get(\"page\", chunk.metadata.get(\"page_number\", \"N/A\"))\n",
    "\n",
    "        # Give each chunk a unique \"Title\" for citation\n",
    "        chunk_title = f\"{title} (Page {page}, Chunk {i+1})\"\n",
    "        chunk.metadata[\"sense_title\"] = chunk_title\n",
    "\n",
    "        content = (\n",
    "            f\"=======================================DOCUMENT METADATA====================================\\n\"\n",
    "            f\"Source: {source}\\n\"\n",
    "            f\"Title: {chunk_title}\\n\"\n",
    "            f\"============================DOCUMENT PAGE CONTENT CHUNK=====================================\\n\"\n",
    "            f\"Page Content Chunk: \\n\\n{chunk.page_content}\\n\\n\"\n",
    "            f\"=====================================================================================\"\n",
    "        )\n",
    "        context_strings.append(content)\n",
    "    return \"\\n\\n\".join(context_strings)\n",
    "\n",
    "def build_advanced_rag_prompt(question: str, context: str) -> str:\n",
    "    \"\"\"Builds an advanced few-shot RAG prompt with IEEE-style citations.\"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a highly advanced AI research assistant. Your task is to answer the user's query based *only* on the provided document chunks.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Read the **USER QUERY** carefully.\n",
    "2.  Analyze the **PROVIDED DOCUMENTS** to find all relevant information.\n",
    "3.  Synthesize a comprehensive, detailed answer that directly addresses the query.\n",
    "4.  **Crucially, you must cite your answer** using IEEE-style in-text citations, like [1], [2].\n",
    "5.  The **Title** and **Source** from the `DOCUMENT METADATA` must be used for citations.\n",
    "6.  You will be forced to output your response as a JSON object with an \"answer\" and \"references\" field.\n",
    "7.  Generate a reference for *every* piece of information you use.\n",
    "8.  **DO NOT** make up information. If the documents do not contain the answer, state that.\n",
    "\n",
    "---\n",
    "**EXAMPLE OF HOW TO CITE:**\n",
    "\n",
    "**[Example] Provided Documents:**\n",
    "=======================================DOCUMENT METADATA====================================\n",
    "Source: https://example.com/ai.pdf\n",
    "Title: AI in 2024 (Page 5, Chunk 1)\n",
    "============================DOCUMENT PAGE CONTENT CHUNK=====================================\n",
    "Page Content Chunk: \\n\\nArtificial intelligence has seen\n",
    "rapid growth, especially in large language models. [1]\n",
    "=====================================================================================\n",
    "=======================================DOCUMENT METADATA====================================\n",
    "Source: https://example.com/ml.pdf\n",
    "Title: ML Basics (Page 2, Chunk 4)\n",
    "============================DOCUMENT PAGE CONTENT CHUNK=====================================\n",
    "Page Content Chunk: \\n\\nMachine learning is a subset of AI.\n",
    "=====================================================================================\n",
    "\n",
    "**[Example] Expected JSON Output:**\n",
    "{{\n",
    "  \"answer\": \"Artificial intelligence (AI) has experienced rapid growth, particularly in the realm of large language models [1]. Machine learning is known to be a subset of AI [2].\",\n",
    "  \"references\": [\n",
    "    {{\n",
    "      \"id\": \"1\",\n",
    "      \"title\": \"AI in 2024 (Page 5, Chunk 1)\",\n",
    "      \"source\": \"https://example.com/ai.pdf\",\n",
    "      \"page\": 5,\n",
    "      \"snippet\": \"Artificial intelligence has seen rapid growth, especially in large language models.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"id\": \"2\",\n",
    "      \"title\": \"ML Basics (Page 2, Chunk 4)\",\n",
    "      \"source\": \"https://example.com/ml.pdf\",\n",
    "      \"page\": 2,\n",
    "      \"snippet\": \"Machine learning is a subset of AI.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "---\n",
    "\n",
    "**ACTUAL TASK:**\n",
    "\n",
    "**USER QUERY:** {question}\n",
    "\n",
    "**PROVIDED DOCUMENTS:**\n",
    "{context}\n",
    "\n",
    "**YOUR JSON RESPONSE:**\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. CORE: HIERARCHICAL RAG SERVICE (MODIFIED)\n",
    "# ==============================================================================\n",
    "\n",
    "class HierarchicalRAGService:\n",
    "    \"\"\"\n",
    "    Manages the Hierarchical Vector Stores (Summary & Detailed)\n",
    "    and all core RAG logic.\n",
    "    \"\"\"\n",
    "    def __init__(self, summary_path, detailed_path, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.summary_store = Chroma(\n",
    "            collection_name=\"summary_store\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=summary_path\n",
    "        )\n",
    "        self.detailed_store = Chroma(\n",
    "            collection_name=\"detailed_store\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=detailed_path\n",
    "        )\n",
    "\n",
    "    # --- Ingestion Logic (Unchanged) ---\n",
    "    def _load_and_split_pdf(self, pdf_bytes: bytes) -> List[Document]:\n",
    "        print(\"[PDF] Creating temporary PDF file...\")\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
    "            tmp.write(pdf_bytes)\n",
    "            path = tmp.name\n",
    "\n",
    "        print(\"[PDF] Loading PDF with PyMuPDF...\")\n",
    "        docs = PyMuPDFLoader(path).load()\n",
    "        print(f\"[PDF] Loaded {len(docs)} pages from PDF\")\n",
    "\n",
    "        if not docs:\n",
    "            print(\"[PDF] No documents loaded from PDF\")\n",
    "            os.remove(path)\n",
    "            return []\n",
    "\n",
    "        print(\"[PDF] Extracting images from PDF...\")\n",
    "        pdf = fitz.open(path)\n",
    "        images_per_page = {}\n",
    "        for page_num in range(len(pdf)):\n",
    "            page = pdf[page_num]\n",
    "            images = page.get_images(full=True)\n",
    "            page_images = []\n",
    "            for img_index, img in enumerate(images):\n",
    "                xref = img[0]\n",
    "                try:\n",
    "                    base_image = pdf.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    image_ext = base_image[\"ext\"]\n",
    "                    img_id = f\"img_{uuid4().hex}\"\n",
    "                    image_filename = f\"{img_id}.{image_ext}\"\n",
    "                    image_path = os.path.join(IMAGE_STORE, image_filename)\n",
    "                    with open(image_path, \"wb\") as f:\n",
    "                        f.write(image_bytes)\n",
    "                    summary = generate_image_summary(image_path)\n",
    "                    page_images.append({\n",
    "                        \"id\": img_id,\n",
    "                        \"summary\": summary,\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"ext\": image_ext\n",
    "                    })\n",
    "                    print(f\"[PDF] Extracted and summarized image {img_id} from page {page_num + 1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[PDF] Failed to extract image {img_index} from page {page_num}: {e}\")\n",
    "            images_per_page[page_num + 1] = page_images\n",
    "        pdf.close()\n",
    "        os.remove(path)\n",
    "        print(f\"[PDF] Completed image extraction, found images on {len([p for p in images_per_page.values() if p])} pages\")\n",
    "\n",
    "        print(\"[PDF] Assigning images to documents and converting to Document objects...\")\n",
    "        # Assign images to documents based on page, and ensure all are Document objects\n",
    "        new_docs = []\n",
    "        for doc in docs:\n",
    "            page_num = getattr(doc, 'metadata', {}).get('page', 1) if hasattr(doc, 'metadata') else 1\n",
    "            images = images_per_page.get(page_num, [])\n",
    "            # If doc is not a Document, convert it\n",
    "            if not isinstance(doc, Document):\n",
    "                doc = Document(page_content=str(doc), metadata={})\n",
    "            # Ensure metadata is a dict\n",
    "            if not hasattr(doc, 'metadata') or not isinstance(doc.metadata, dict):\n",
    "                doc.metadata = {}\n",
    "            doc.metadata[\"images\"] = images\n",
    "            new_docs.append(doc)\n",
    "\n",
    "        print(\"[PDF] Splitting documents into semantic chunks...\")\n",
    "        splitter = SemanticChunker(self.embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "        chunks = splitter.split_documents(new_docs)\n",
    "        print(f\"[PDF] Split into {len(chunks)} semantic chunks\")\n",
    "\n",
    "        # Ensure all chunks are Document objects\n",
    "        safe_chunks = []\n",
    "        for c in chunks:\n",
    "            if isinstance(c, Document):\n",
    "                safe_chunks.append(c)\n",
    "            else:\n",
    "                safe_chunks.append(Document(page_content=str(c), metadata={}))\n",
    "        print(f\"[PDF] Final chunk count: {len(safe_chunks)}\")\n",
    "        return safe_chunks\n",
    "\n",
    "    async def _generate_summary_for_ingestion(self, chunks: List[Document], model_name: str) -> str:\n",
    "        print(f\"[SUMMARY] Starting summary generation for {len(chunks)} chunks\")\n",
    "        if not chunks:\n",
    "            print(\"[SUMMARY] No chunks provided for summary\")\n",
    "            return \"No text content found.\"\n",
    "\n",
    "        print(f\"[SUMMARY] Processing chunks in batches of 5...\")\n",
    "        async def summarize_chunk_async(chunk_text: str) -> str:\n",
    "            prompt = f\"Summarize the following text chunk in 2-3 key bullet points:\\n\\n{chunk_text}\\n\\nSummary:\"\n",
    "            return await asyncio.to_thread(generate_with_llm, prompt, model_name)\n",
    "\n",
    "        intermediate_summaries = []\n",
    "        batch_count = (len(chunks) + 4) // 5  # Ceiling division\n",
    "        for i in range(0, len(chunks), 5):\n",
    "            batch_num = i // 5 + 1\n",
    "            print(f\"[SUMMARY] Processing batch {batch_num}/{batch_count}\")\n",
    "            batch = chunks[i:i+5]\n",
    "            tasks = [asyncio.create_task(summarize_chunk_async(c.page_content)) for c in batch]\n",
    "            batch_summaries = await asyncio.gather(*tasks)\n",
    "            intermediate_summaries.extend(batch_summaries)\n",
    "            print(f\"[SUMMARY] Completed batch {batch_num}, {len(batch_summaries)} summaries generated\")\n",
    "\n",
    "        print(f\"[SUMMARY] Synthesizing {len(intermediate_summaries)} intermediate summaries...\")\n",
    "        combined_summaries = \"\\n\".join(intermediate_summaries)\n",
    "        synthesis_prompt = (\n",
    "            f\"Create a single, concise paragraph summarizing the key themes \"\n",
    "            f\"from the following list of chunk summaries.\\n\\nSummaries:\\n{combined_summaries}\\n\\nOverall Summary Paragraph:\"\n",
    "        )\n",
    "        final_summary = await asyncio.to_thread(generate_with_llm, synthesis_prompt, model_name)\n",
    "        print(f\"[SUMMARY] Final summary generated: {len(final_summary)} characters\")\n",
    "        return final_summary\n",
    "\n",
    "    async def add_document_to_stores(self, pdf_bytes: bytes, doc_id: str, model_name: str):\n",
    "        print(f\"[RAG] Starting document ingestion for doc_id: {doc_id}\")\n",
    "        post_progress(doc_id, \"loading\", 5, message=\"Loading PDF...\")\n",
    "\n",
    "        print(\"[RAG] Loading and splitting PDF...\")\n",
    "        chunks = self._load_and_split_pdf(pdf_bytes)\n",
    "        print(f\"[RAG] Split into {len(chunks)} chunks\")\n",
    "\n",
    "        # Diagnostic logging: ensure chunks is a list of Document objects\n",
    "        try:\n",
    "            print(f\"[RAG] Chunk diagnostics: {len(chunks)} total chunks\")\n",
    "            for i, c in enumerate(chunks[:3]):\n",
    "                print(f\"[RAG] Chunk {i}: type={type(c)}, has_metadata={hasattr(c, 'metadata')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[RAG] Could not inspect chunks: {e}\")\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"[RAG] No text content found in document\")\n",
    "            post_progress(doc_id, \"failed\", 0, message=\"No text content found\")\n",
    "            raise ValueError(\"No text content found in the document\")\n",
    "\n",
    "        print(f\"[RAG] Successfully split into {len(chunks)} chunks\")\n",
    "        post_progress(doc_id, \"chunking\", 20, message=f\"Split into {len(chunks)} chunks\", totalChunks=len(chunks))\n",
    "\n",
    "        # Safely resolve source filename\n",
    "        if chunks and hasattr(chunks[0], 'metadata') and isinstance(chunks[0].metadata, dict):\n",
    "            source_filename = chunks[0].metadata.get(\"source\", f\"doc_{doc_id}\")\n",
    "        else:\n",
    "            print(f\"[RAG] First chunk missing metadata, using fallback source filename\")\n",
    "            source_filename = f\"doc_{doc_id}\"\n",
    "\n",
    "        print(\"[RAG] Generating document summary...\")\n",
    "        post_progress(doc_id, \"summarizing\", 40, message=\"Generating document summary...\", totalChunks=len(chunks))\n",
    "        summary_text = await self._generate_summary_for_ingestion(chunks, model_name)\n",
    "        print(f\"[RAG] Summary generated: {len(summary_text)} characters\")\n",
    "\n",
    "        print(\"[RAG] Creating summary embeddings...\")\n",
    "        post_progress(doc_id, \"embedding_summary\", 60, message=\"Creating summary embeddings...\", totalChunks=len(chunks))\n",
    "        summary_doc = Document(\n",
    "            page_content=summary_text,\n",
    "            metadata={\"doc_id\": doc_id, \"source\": source_filename, \"title\": f\"Summary for {source_filename}\"}\n",
    "        )\n",
    "        self.summary_store.add_documents([summary_doc], ids=[doc_id])\n",
    "        print(\"[RAG] Summary embeddings created and stored\")\n",
    "\n",
    "        print(\"[RAG] Creating chunk embeddings...\")\n",
    "        post_progress(doc_id, \"embedding_chunks\", 75, message=\"Creating chunk embeddings...\", totalChunks=len(chunks))\n",
    "        current_index = 0\n",
    "        for i in range(0, len(chunks), 5):\n",
    "            batch = chunks[i:i+5]\n",
    "            print(f\"[RAG] Processing batch {i//5 + 1}/{(len(chunks) + 4)//5}\")\n",
    "            for chunk in batch:\n",
    "                # Defensive conversion: ensure chunk is Document and has dict metadata\n",
    "                if not isinstance(chunk, Document):\n",
    "                    print(f\"[RAG] Converting non-Document chunk at index {current_index}\")\n",
    "                    chunk = Document(page_content=str(chunk), metadata={})\n",
    "                    chunks[current_index] = chunk\n",
    "                if not hasattr(chunk, 'metadata') or not isinstance(chunk.metadata, dict):\n",
    "                    chunk.metadata = {}\n",
    "                chunk.metadata[\"doc_id\"] = doc_id\n",
    "                chunk.metadata[\"title\"] = f\"{Path(source_filename).name} (Page {chunk.metadata.get('page', current_index+1)})\"\n",
    "                try:\n",
    "                    # san is sanitized so lists/dicts converted to strings\n",
    "                    chunk.metadata = sanitize_metadata(chunk.metadata)\n",
    "                except Exception as e:\n",
    "                    print(f\"[RAG] sanitize_metadata failed for chunk {current_index}: {e}\")\n",
    "                current_index += 1\n",
    "            progress = 75 + int(current_index / len(chunks) * 20)\n",
    "            post_progress(doc_id, \"embedding_chunks\", progress,\n",
    "                        message=f\"Embedding chunk {current_index}/{len(chunks)}...\",\n",
    "                        currentChunk=current_index, totalChunks=len(chunks))\n",
    "            print(f\"[RAG] Completed batch, {current_index}/{len(chunks)} chunks processed\")\n",
    "\n",
    "        print(\"[RAG] Final metadata sanitization...\")\n",
    "        # Final sanitization before sending to Chroma: ensure every metadata value is primitive\n",
    "        for i, ch in enumerate(chunks):\n",
    "            if not hasattr(ch, 'metadata') or not isinstance(ch.metadata, dict):\n",
    "                ch.metadata = {}\n",
    "            try:\n",
    "                ch.metadata = sanitize_metadata(ch.metadata)\n",
    "            except Exception as e:\n",
    "                print(f\"[RAG] Final sanitize_metadata failed for chunk {i}: {e}\")\n",
    "                ch.metadata = {}\n",
    "\n",
    "        print(\"[RAG] Adding chunks to detailed store...\")\n",
    "        chunk_ids = [f\"{doc_id}_{i}\" for i in range(len(chunks))]\n",
    "        try:\n",
    "            self.detailed_store.add_documents(chunks, ids=chunk_ids)\n",
    "            print(f\"[RAG] Successfully added {len(chunks)} chunks to detailed store\")\n",
    "        except Exception as e:\n",
    "            print(f\"[RAG] Failed to add documents to detailed_store: {e}\")\n",
    "            post_progress(doc_id, \"failed\", 0, message=f\"Failed to add documents: {e}\")\n",
    "            raise\n",
    "\n",
    "        print(f\"[RAG] Document processing complete! {len(chunks)} chunks processed\")\n",
    "        post_progress(doc_id, \"complete\", 100, message=\"Document processing complete!\", totalChunks=len(chunks))\n",
    "        return len(chunks)\n",
    "\n",
    "    def get_chunks_by_doc_id(self, doc_id: str) -> List[Document]:\n",
    "        results = self.detailed_store.get(where={\"doc_id\": doc_id}, include=[\"metadatas\", \"documents\"])\n",
    "        print(f\"DEBUG: detailed_store.get returned keys: {list(results.keys())}\")\n",
    "        try:\n",
    "            # Print types for debugging\n",
    "            print(f\"DEBUG: metadatas type={type(results.get('metadatas'))}, documents type={type(results.get('documents'))}\")\n",
    "            if isinstance(results.get('documents'), list):\n",
    "                for i, d in enumerate(results.get('documents')[:5]):\n",
    "                    print(f\"DEBUG doc {i} type={type(d)} content_preview={str(d)[:80]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"DEBUG: could not inspect results from detailed_store.get: {e}\")\n",
    "        if not results.get('documents') or not results.get('metadatas'):\n",
    "            return []\n",
    "        docs = []\n",
    "        for i, text in enumerate(results['documents']):\n",
    "            if i >= len(results['metadatas']):\n",
    "                continue\n",
    "            meta = results['metadatas'][i]\n",
    "            try:\n",
    "                if isinstance(text, str) and isinstance(meta, dict):\n",
    "                    sanitized_meta = sanitize_metadata(meta)\n",
    "                    docs.append(Document(page_content=text, metadata=sanitized_meta))\n",
    "                else:\n",
    "                    print(f\"Warning: unexpected type for document {i}: text={type(text)}, meta={type(meta)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating document {i}: {e}\")\n",
    "                continue\n",
    "        return docs\n",
    "\n",
    "    # --- RAG Logic with Structured Citations ---\n",
    "    async def query_rag(self, document_ids: List[str], question: str, model_name: str, top_k: int = 5, specific_chunks: Dict[str, List[int]] = None) -> Tuple[str, List[Dict[str, Any]]]:\n",
    "\n",
    "        summary_retriever = self.summary_store.as_retriever(search_kwargs={'k': 20, 'filter': {'doc_id': {'$in': document_ids}}})\n",
    "        summary_compressor = ContextualCompressionRetriever(base_compressor=GLOBAL_RERANKER, base_retriever=summary_retriever)\n",
    "        relevant_summaries = summary_compressor.invoke(question)\n",
    "        relevant_doc_ids = []\n",
    "        for doc in relevant_summaries:\n",
    "            if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict) and 'doc_id' in doc.metadata:\n",
    "                relevant_doc_ids.append(doc.metadata['doc_id'])\n",
    "        relevant_doc_ids = list(set(relevant_doc_ids))\n",
    "\n",
    "        if not relevant_doc_ids:\n",
    "            return \"No relevant documents found.\", []\n",
    "\n",
    "        if specific_chunks:\n",
    "            relevant_chunks = []\n",
    "            for doc_id in relevant_doc_ids:\n",
    "                if doc_id in specific_chunks:\n",
    "                    all_chunks = self.get_chunks_by_doc_id(doc_id)\n",
    "                    selected_indices = specific_chunks[doc_id]\n",
    "                    for idx in selected_indices:\n",
    "                        if idx < len(all_chunks):\n",
    "                            relevant_chunks.append(all_chunks[idx])\n",
    "        else:\n",
    "            detailed_retriever = self.detailed_store.as_retriever(search_kwargs={'k': 25, 'filter': {'doc_id': {'$in': relevant_doc_ids}}})\n",
    "            chunk_compressor = ContextualCompressionRetriever(base_compressor=FlashrankRerank(top_n=top_k), base_retriever=detailed_retriever)\n",
    "            relevant_chunks = chunk_compressor.invoke(question)\n",
    "            # Ensure relevant_chunks are Document objects\n",
    "            for i, rc in enumerate(relevant_chunks):\n",
    "                if not isinstance(rc, Document):\n",
    "                    print(f\"Converting non-Document relevant chunk at index {i} of type {type(rc)}\")\n",
    "                    relevant_chunks[i] = Document(page_content=str(rc), metadata={})\n",
    "            print(f\"DEBUG: relevant_chunks types after conversion: {[type(rc) for rc in relevant_chunks[:5]]}\")\n",
    "\n",
    "        if not relevant_chunks:\n",
    "            return \"No relevant chunks found.\", []\n",
    "\n",
    "        context_string = _format_chunks_for_prompt(relevant_chunks)\n",
    "        final_prompt = build_advanced_rag_prompt(question, context_string)\n",
    "\n",
    "        llm_name_for_rag = model_name\n",
    "        if model_name.lower() not in [\"gemma3:1b\", \"llama3\"]:\n",
    "            llm_name_for_rag = \"gemma3:1b\"\n",
    "\n",
    "        try:\n",
    "            llm = get_llm(llm_name_for_rag)\n",
    "            json_prompt = final_prompt + \"\"\"\\n\\nReturn JSON: {\"answer\": \"...\", \"references\": [{\"id\": \"1\", \"title\": \"...\", \"source\": \"...\", \"page\": 1, \"snippet\": \"...\"}]}\"\"\"\n",
    "            raw_response = await asyncio.to_thread(llm.invoke, json_prompt)\n",
    "            raw_response_content = getattr(raw_response, \"content\", str(raw_response))\n",
    "            print(f\"LLM response received (length: {len(raw_response_content)} chars)\")\n",
    "            print(f\"Raw response preview: {raw_response_content[:300]}...\")\n",
    "\n",
    "            import json, re\n",
    "            json_match = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', raw_response_content, re.DOTALL) or re.search(r'\\{.*\"answer\".*\"references\".*\\}', raw_response_content, re.DOTALL)\n",
    "            if not json_match:\n",
    "                raise ValueError(\"No JSON in response\")\n",
    "            parsed_json = json.loads(json_match.group(1) if json_match.groups() else json_match.group(0))\n",
    "            references = [Reference(id=str(ref.get('id', i+1)), title=ref.get('title', 'Unknown'), source=ref.get('source', 'Unknown'), page=ref.get('page', 0), snippet=ref.get('snippet', '')) for i, ref in enumerate(parsed_json.get('references', []))]\n",
    "            ai_answer_response = AIAnswer(answer=parsed_json.get('answer', ''), references=references)\n",
    "\n",
    "            final_answer, final_refs = deduplicate_references_and_update_answer(ai_answer_response.answer, ai_answer_response.references)\n",
    "\n",
    "            chunk_map = {}\n",
    "            for chunk in relevant_chunks:\n",
    "                if hasattr(chunk, 'metadata') and isinstance(chunk.metadata, dict):\n",
    "                    source = chunk.metadata.get(\"source\", \"\")\n",
    "                    if source:\n",
    "                        chunk_map[source] = chunk\n",
    "            final_refs_dict = []\n",
    "            for i, ref in enumerate(final_refs):\n",
    "                chunk = chunk_map.get(ref.source)\n",
    "                page = ref.page or (chunk.metadata.get(\"page\", 0) if chunk and hasattr(chunk, 'metadata') and isinstance(chunk.metadata, dict) else 0)\n",
    "                snippet = ref.snippet or (chunk.page_content[:200] if chunk else \"\")\n",
    "                full_text = chunk.page_content if chunk else ref.title\n",
    "                final_refs_dict.append({\n",
    "                    \"documentId\": ref.source.split(\"/\")[-1] if \"/\" in ref.source else ref.source,\n",
    "                    \"page\": page,\n",
    "                    \"snippet\": snippet,\n",
    "                    \"fullText\": full_text,\n",
    "                    \"source\": ref.title,\n",
    "                    \"rank\": i + 1,\n",
    "                    \"score\": None\n",
    "                })\n",
    "\n",
    "            return final_answer, final_refs_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            simple_context = \"\\n\\n\".join([c.page_content for c in relevant_chunks])\n",
    "            simple_prompt = f\"Answer: {question}\\n\\nContext:\\n{simple_context}\\n\\nAnswer:\"\n",
    "            return generate_with_llm(simple_prompt, model_name), []\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. STREAMING SUMMARIZER (Preserved Feature)\n",
    "# ==============================================================================\n",
    "# (This section is unchanged from the previous code)\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. FASTAPI APP & ENDPOINTS (MODIFIED)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Starting FastAPI app...\")\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    global rag_service\n",
    "    try:\n",
    "        rag_service = HierarchicalRAGService(\n",
    "            summary_path=SUMMARY_STORE_PATH,\n",
    "            detailed_path=DETAILED_STORE_PATH,\n",
    "            embeddings=EMBEDDINGS_MODEL\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not initialize RAG Service: {e}\")\n",
    "        rag_service = None\n",
    "    yield\n",
    "    # Shutdown code (if needed)\n",
    "\n",
    "app = FastAPI(title=\"Hierarchical RAG API with Structured Citations\", lifespan=lifespan)\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  \n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\"status\": \"healthy\", \"message\": \"Backend is running\"}\n",
    "\n",
    "@app.post(\"/process\")\n",
    "async def process(file: UploadFile):\n",
    "    print(\"[PROCESS] Starting document processing...\")\n",
    "    if rag_service is None:\n",
    "        print(\"[PROCESS] RAG Service is not operational\")\n",
    "        raise HTTPException(status_code=500, detail=\"RAG Service is not operational.\")\n",
    "\n",
    "    print(f\"[PROCESS] Received file: {file.filename}\")\n",
    "    if is_image_file(file.filename):\n",
    "        print(\"[PROCESS] Detected image file\")\n",
    "        doc_id = f\"img_{uuid4().hex}\"\n",
    "        image_path = os.path.join(IMAGE_STORE, f\"{doc_id}{Path(file.filename).suffix}\")\n",
    "        with open(image_path, \"wb\") as f: f.write(await file.read())\n",
    "        print(f\"[PROCESS] Image saved: {image_path}\")\n",
    "        return {\"documentId\": doc_id, \"status\": \"image_saved\", \"isImage\": True, \"imagePath\": image_path}\n",
    "\n",
    "    print(\"[PROCESS] Processing as PDF document\")\n",
    "    doc_id = f\"doc_{uuid4().hex}\"\n",
    "    print(f\"[PROCESS] Generated document ID: {doc_id}\")\n",
    "\n",
    "    try:\n",
    "        print(\"[PROCESS] Reading PDF bytes...\")\n",
    "        pdf_bytes = await file.read()\n",
    "        print(f\"[PROCESS] Read {len(pdf_bytes)} bytes\")\n",
    "\n",
    "        print(\"[PROCESS] Calling add_document_to_stores...\")\n",
    "        chunk_count = await rag_service.add_document_to_stores(pdf_bytes, doc_id, OLLAMA_MODEL)\n",
    "        print(f\"[PROCESS] Successfully processed document with {chunk_count} chunks\")\n",
    "        return {\"documentId\": doc_id, \"status\": \"embeddings_created\", \"chunkCount\": chunk_count, \"isImage\": False}\n",
    "    except Exception as e:\n",
    "        print(f\"[PROCESS] Error processing document: {e}\")\n",
    "        return JSONResponse(status_code=500, content={\"error\": \"Failed to process document\", \"message\": str(e)})\n",
    "\n",
    "@app.post(\"/get_chunks\")\n",
    "async def get_chunks(request: Request):\n",
    "    \"\"\"Get all chunks for a specific document ID.\"\"\"\n",
    "    if rag_service is None: raise HTTPException(status_code=500, detail=\"RAG Service is not operational.\")\n",
    "    data = await request.json()\n",
    "    document_id = data.get(\"documentId\")\n",
    "    if not document_id: raise HTTPException(status_code=400, detail=\"documentId is required\")\n",
    "    chunks = rag_service.get_chunks_by_doc_id(document_id)\n",
    "    if not chunks: raise HTTPException(status_code=404, detail=f\"No chunks found for documentId {document_id}\")\n",
    "    def _parse_images_field(md):\n",
    "        try:\n",
    "            if not md: return []\n",
    "            imgs = md.get(\"images\")\n",
    "            if imgs is None:\n",
    "                return []\n",
    "            if isinstance(imgs, str):\n",
    "                try:\n",
    "                    parsed = json.loads(imgs)\n",
    "                    return parsed if isinstance(parsed, list) else [parsed]\n",
    "                except Exception:\n",
    "                    return []\n",
    "            elif isinstance(imgs, list):\n",
    "                return imgs\n",
    "            else:\n",
    "                return []\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    return JSONResponse(content={\n",
    "        \"documentId\": document_id,\n",
    "        \"chunks\": [\n",
    "            {\n",
    "                \"id\": i,\n",
    "                \"content\": chunk.page_content,\n",
    "                \"metadata\": chunk.metadata if hasattr(chunk, 'metadata') and isinstance(chunk.metadata, dict) else {},\n",
    "                \"images\": _parse_images_field(chunk.metadata if hasattr(chunk, 'metadata') and isinstance(chunk.metadata, dict) else {})\n",
    "            }\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate_text(request: Request):\n",
    "    \"\"\"\n",
    "    Main RAG endpoint with hierarchical retrieval and structured citations.\n",
    "    \"\"\"\n",
    "    if rag_service is None: raise HTTPException(status_code=500, detail=\"RAG Service is not operational.\")\n",
    "\n",
    "    data = await request.json()\n",
    "    model = data.get(\"model\", OLLAMA_MODEL)\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "    document_ids = data.get(\"documentIds\", [])\n",
    "    specific_chunks = data.get(\"specificChunks\", None)  # Optional: {\"doc_id\": [0, 2, 5], ...}\n",
    "\n",
    "    image_ids = [doc_id for doc_id in document_ids if doc_id.startswith(\"img_\")]\n",
    "    text_ids = [doc_id for doc_id in document_ids if doc_id.startswith(\"doc_\")]\n",
    "\n",
    "    if image_ids:\n",
    "        # Image Q&A logic is preserved\n",
    "        return await process_image_query(image_ids, text_ids, prompt, model)\n",
    "\n",
    "    if text_ids:\n",
    "        # --- Call the advanced RAG function ---\n",
    "        max_citations = 7 # Get more chunks for the advanced prompt\n",
    "\n",
    "        # Use await because query_rag is now an async function\n",
    "        response_text, citations = await rag_service.query_rag(\n",
    "            document_ids=text_ids,\n",
    "            question=prompt,\n",
    "            model_name=model,\n",
    "            specific_chunks=specific_chunks,\n",
    "            top_k=max_citations\n",
    "        )\n",
    "        \n",
    "        num_citations = random.randint(2, 3)\n",
    "        citations = citations[:num_citations]\n",
    "        return JSONResponse(content={\"response\": response_text, \"citations\": citations})\n",
    "\n",
    "    # --- No-context Q&A Logic (Unchanged) ---\n",
    "    response_text = generate_with_llm(prompt, model) # Uses simple text gen\n",
    "    return JSONResponse(content={\"response\": response_text, \"citations\": []})\n",
    "\n",
    "\n",
    "async def process_image_query(image_ids: list, text_ids: list, prompt: str, model: str):\n",
    "    \"\"\"\n",
    "    Image Q&A function with optional RAG context from text documents.\n",
    "    \"\"\"\n",
    "    vision_model = \"llava\"\n",
    "    print(f\"Image queries: forcing vision model='{vision_model}', ignoring requested model='{model}'\")\n",
    "    responses = []\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        image_files = [f for f in os.listdir(IMAGE_STORE) if f.startswith(img_id)]\n",
    "        if not image_files:\n",
    "            responses.append(f\"Image {img_id} not found.\")\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(IMAGE_STORE, image_files[0])\n",
    "        with open(image_path, \"rb\") as f: image_data = base64.b64encode(f.read()).decode()\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{OLLAMA_URL}/api/generate\",\n",
    "                json={\"model\": vision_model, \"prompt\": prompt, \"images\": [image_data], \"stream\": False},\n",
    "                timeout=120\n",
    "            )\n",
    "            if response.status_code == 200: responses.append(response.json().get(\"response\", \"No response\"))\n",
    "            else: responses.append(f\"Error: Vision model status {response.status_code}\")\n",
    "        except Exception as e: responses.append(f\"Error processing image: {str(e)}\")\n",
    "\n",
    "    additional_context = \"\"\n",
    "    citations = []\n",
    "\n",
    "    if text_ids and rag_service:\n",
    "        print(\"... Image query also performing RAG on text documents ...\")\n",
    "        # Await the async RAG query\n",
    "        context, citations = await rag_service.query_rag(text_ids, prompt, model, top_k=3)\n",
    "        num_citations = random.randint(2, 3)\n",
    "        citations = citations[:num_citations]\n",
    "        additional_context = f\"\\n\\nAdditional context from documents:\\n{context}\"\n",
    "\n",
    "    final_response = \"\\n\\n\".join(responses)\n",
    "    if additional_context: final_response += additional_context\n",
    "\n",
    "    return JSONResponse(content={\"response\": final_response, \"citations\": citations, \"usedVisionModel\": True, \"visionModel\": vision_model})\n",
    "\n",
    "\n",
    "\n",
    "from fastapi.responses import Response\n",
    "\n",
    "@app.get(\"/image_bytes/{image_id}\")\n",
    "async def get_image_bytes(image_id: str):\n",
    "    \"\"\"Serve an image by its ID as bytes.\"\"\"\n",
    "    for ext in IMAGE_EXTENSIONS:\n",
    "        image_path = os.path.join(IMAGE_STORE, f\"{image_id}{ext}\")\n",
    "        if os.path.exists(image_path):\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                data = f.read()\n",
    "            return Response(data, media_type=f\"image/{ext[1:]}\")\n",
    "    raise HTTPException(status_code=404, detail=\"Image not found\")\n",
    "\n",
    "\n",
    "@app.post(\"/pull\")\n",
    "async def pull_model(request: Request):\n",
    "    model = (await request.json()).get(\"name\", OLLAMA_MODEL)\n",
    "    resp = requests.post(f\"{OLLAMA_URL}/api/pull\", json={\"name\": model, \"stream\": False}, timeout=300)\n",
    "    return JSONResponse(content=resp.json(), status_code=resp.status_code)\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. SERVER STARTUP\n",
    "# ==============================================================================\n",
    "\n",
    "try:\n",
    "    # Pull models first before starting service\n",
    "    print(\"Pulling required models...\")\n",
    "    os.system(\"ollama pull gemma3:1b\")\n",
    "    os.system(\"ollama pull llava\")\n",
    "    start_ollama_service()\n",
    "    os.system(\"ollama pull gemma3:1b && ollama pull llava\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to start Ollama service: {e}\")\n",
    "    raise\n",
    "\n",
    "FIXED_URL = \"https://mari-unbequeathed-milkily.ngrok-free.app\"\n",
    "public_url = \"http://localhost:8000\"\n",
    "ngrok_enabled = False\n",
    "\n",
    "if NGROK_AUTHTOKEN and NGROK_AUTHTOKEN != \"YOUR_NGROK_AUTHTOKEN\":\n",
    "    os.system(f\"ngrok config add-authtoken {NGROK_AUTHTOKEN}\")\n",
    "    ngrok_proc = subprocess.Popen([\"ngrok\", \"http\", \"--host-header=rewrite\", \"--url\", FIXED_URL, \"8000\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    public_url = FIXED_URL\n",
    "    ngrok_enabled = True\n",
    "    time.sleep(3)\n",
    "    print(f\"Public URL: {public_url}\")\n",
    "\n",
    "config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\", access_log=True)\n",
    "server = uvicorn.Server(config)\n",
    "\n",
    "threading.Thread(target=lambda: asyncio.run(server.serve()), daemon=True).start()\n",
    "\n",
    "try:\n",
    "    while True: time.sleep(300)\n",
    "except KeyboardInterrupt:\n",
    "    if ngrok_enabled and ngrok_proc:\n",
    "        ngrok_proc.terminate()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
